{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 20:21:09,398 - INFO - Fetching O*NET Crosswalk data from https://raw.githubusercontent.com/Kyleinexile/IS-Repo/refs/heads/main/ONET%20Crosswalk.csv\n",
      "2025-03-27 20:21:09,539 - INFO - Retrieved crosswalk data, size: 8736712 bytes\n",
      "2025-03-27 20:21:09,543 - INFO - Parsing and filtering crosswalk data\n",
      "2025-03-27 20:21:09,680 - INFO - Parsed 40077 entries from CSV\n",
      "2025-03-27 20:21:09,685 - INFO - Service type counts: {'V': 4419, 'N': 7302, 'J': 809, 'C': 320, 'G': 1150, 'F': 9624, 'H': 4517, 'Y': 3418, 'U': 2377, 'A': 2086, 'X': 70, 'M': 2245, 'O': 20, 'S': 562, 'P': 208, 'D': 467, 'Q': 54, 'Z': 40, 'K': 343, 'L': 46}\n",
      "2025-03-27 20:21:09,693 - INFO - Found 7271 Air Force and 6408 Space Force entries\n",
      "2025-03-27 20:21:09,758 - INFO - Extracting keywords from AFSC entries\n",
      "2025-03-27 20:21:09,827 - INFO - Keyword extraction complete. Processed 13679 entries.\n",
      "2025-03-27 20:21:10,020 - INFO - Data saved to processed_data\\processed-afsc-crosswalk-2025-03-27-20-21-09.json\n",
      "2025-03-27 20:21:10,020 - INFO - Workflow completed successfully\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "O*NET Military Crosswalk Processing Pipeline\n",
    "\n",
    "This script replicates the n8n workflow for processing O*NET Military Crosswalk data:\n",
    "1. Fetches raw crosswalk data from GitHub\n",
    "2. Parses the CSV format\n",
    "3. Filters for Air Force specialty codes\n",
    "4. Extracts keywords and creates searchable text\n",
    "5. Saves the processed data to a local file\n",
    "\n",
    "Author: Kyle Hall\n",
    "Date: March 2025\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "import json\n",
    "import datetime\n",
    "import re\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import logging\n",
    "from io import StringIO\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"onet_crosswalk_processor.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def fetch_crosswalk_data(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch O*NET Military Crosswalk data from GitHub or other URL\n",
    "    \n",
    "    Args:\n",
    "        url: URL to the Crosswalk CSV data\n",
    "        \n",
    "    Returns:\n",
    "        CSV data as string\n",
    "    \"\"\"\n",
    "    logger.info(f\"Fetching O*NET Crosswalk data from {url}\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    logger.info(f\"Retrieved crosswalk data, size: {len(response.text)} bytes\")\n",
    "    return response.text\n",
    "\n",
    "def parse_and_filter_csv(csv_data: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse CSV data and filter for Air Force entries\n",
    "    \n",
    "    Args:\n",
    "        csv_data: Raw CSV data as string\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with processed entries and metadata\n",
    "    \"\"\"\n",
    "    logger.info(\"Parsing and filtering crosswalk data\")\n",
    "    \n",
    "    # Parse CSV\n",
    "    csv_reader = csv.reader(StringIO(csv_data))\n",
    "    headers = next(csv_reader)\n",
    "    headers = [h.strip() for h in headers]\n",
    "    \n",
    "    # Process all rows\n",
    "    parsed_data = []\n",
    "    for row in csv_reader:\n",
    "        if not row:\n",
    "            continue\n",
    "            \n",
    "        entry = {}\n",
    "        for i, value in enumerate(row):\n",
    "            if i < len(headers):\n",
    "                entry[headers[i]] = value.strip()\n",
    "        \n",
    "        parsed_data.append(entry)\n",
    "    \n",
    "    logger.info(f\"Parsed {len(parsed_data)} entries from CSV\")\n",
    "    \n",
    "    # Count service types\n",
    "    service_counts = {}\n",
    "    for item in parsed_data:\n",
    "        svc = item.get('SVC', 'unknown')\n",
    "        service_counts[svc] = service_counts.get(svc, 0) + 1\n",
    "    \n",
    "    logger.info(f\"Service type counts: {service_counts}\")\n",
    "    \n",
    "    # Filter for Air Force entries\n",
    "    air_force_data = [\n",
    "        item for item in parsed_data \n",
    "        if item.get('SVC') in ['F', 'X', 'Y', 'Z'] and item.get('STATUS') == 'A'\n",
    "    ]\n",
    "    \n",
    "    # Filter for Space Force entries\n",
    "    space_force_data = [\n",
    "        item for item in parsed_data \n",
    "        if item.get('SVC') in ['H', 'L', 'O', 'U'] and item.get('STATUS') == 'A'\n",
    "    ]\n",
    "    \n",
    "    logger.info(f\"Found {len(air_force_data)} Air Force and {len(space_force_data)} Space Force entries\")\n",
    "    \n",
    "    # Transform Air Force data\n",
    "    transformed_data = []\n",
    "    \n",
    "    # Process Air Force entries\n",
    "    for item in air_force_data:\n",
    "        transformed_data.append({\n",
    "            'afscCode': item.get('MOC', ''),\n",
    "            'afscTitle': item.get('MOC_TITLE', ''),\n",
    "            'serviceType': item.get('SVC', ''),\n",
    "            'category': 'Enlisted' if item.get('MPC') == 'E' else \n",
    "                        'Officer' if item.get('MPC') == 'O' else 'Warrant',\n",
    "            'onetCodes': [item.get(f'ONET{i}', '') for i in range(1, 5) if item.get(f'ONET{i}')],\n",
    "            'onetTitles': [item.get(f'ONET{i}_TITLE', '') for i in range(1, 5) if item.get(f'ONET{i}_TITLE')],\n",
    "            'socCodes': [item.get(f'SOC{i}', '') for i in range(1, 5) if item.get(f'SOC{i}')],\n",
    "            'socTitles': [item.get(f'SOC{i}_TITLE', '') for i in range(1, 5) if item.get(f'SOC{i}_TITLE')],\n",
    "            'statusDate': item.get('SDATE', ''),\n",
    "            'endDate': item.get('EDATE', '')\n",
    "        })\n",
    "    \n",
    "    # Process Space Force entries\n",
    "    for item in space_force_data:\n",
    "        transformed_data.append({\n",
    "            'afscCode': item.get('MOC', ''),\n",
    "            'afscTitle': item.get('MOC_TITLE', ''),\n",
    "            'serviceType': item.get('SVC', ''),\n",
    "            'category': 'Enlisted' if item.get('MPC') == 'E' else \n",
    "                        'Officer' if item.get('MPC') == 'O' else 'Warrant',\n",
    "            'isSpaceForce': True,\n",
    "            'onetCodes': [item.get(f'ONET{i}', '') for i in range(1, 5) if item.get(f'ONET{i}')],\n",
    "            'onetTitles': [item.get(f'ONET{i}_TITLE', '') for i in range(1, 5) if item.get(f'ONET{i}_TITLE')],\n",
    "            'socCodes': [item.get(f'SOC{i}', '') for i in range(1, 5) if item.get(f'SOC{i}')],\n",
    "            'socTitles': [item.get(f'SOC{i}_TITLE', '') for i in range(1, 5) if item.get(f'SOC{i}_TITLE')],\n",
    "            'statusDate': item.get('SDATE', ''),\n",
    "            'endDate': item.get('EDATE', '')\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'processedCrosswalk': transformed_data,\n",
    "        'metadata': {\n",
    "            'totalEntries': len(parsed_data),\n",
    "            'airForceEntries': len(air_force_data),\n",
    "            'spaceForceEntries': len(space_force_data),\n",
    "            'processingDate': datetime.datetime.now().isoformat()\n",
    "        }\n",
    "    }\n",
    "\n",
    "def extract_keywords(data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract keywords and create search text for each AFSC\n",
    "    \n",
    "    Args:\n",
    "        data: Parsed and filtered crosswalk data\n",
    "        \n",
    "    Returns:\n",
    "        Enhanced data with keywords and search text\n",
    "    \"\"\"\n",
    "    logger.info(\"Extracting keywords from AFSC entries\")\n",
    "    afsc_entries = data['processedCrosswalk']\n",
    "    \n",
    "    processed_entries = []\n",
    "    for afsc in afsc_entries:\n",
    "        # Extract words from titles\n",
    "        title_words = re.split(r'\\s+', afsc.get('afscTitle', '').lower())\n",
    "        \n",
    "        # Extract words from O*NET titles\n",
    "        onet_words = []\n",
    "        for title in afsc.get('onetTitles', []):\n",
    "            if title:\n",
    "                onet_words.extend(re.split(r'\\s+', title.lower()))\n",
    "        \n",
    "        # Extract words from SOC titles\n",
    "        soc_words = []\n",
    "        for title in afsc.get('socTitles', []):\n",
    "            if title:\n",
    "                soc_words.extend(re.split(r'\\s+', title.lower()))\n",
    "        \n",
    "        # Combine all words and remove common words\n",
    "        common_words = ['and', 'or', 'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'with', 'by', 'of']\n",
    "        all_words = [word for word in title_words + onet_words + soc_words \n",
    "                    if word and len(word) > 2 and word not in common_words]\n",
    "        \n",
    "        # Count word frequencies\n",
    "        word_freq = {}\n",
    "        for word in all_words:\n",
    "            word_freq[word] = word_freq.get(word, 0) + 1\n",
    "        \n",
    "        # Sort by frequency and get top keywords\n",
    "        keywords = [word for word, _ in sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:30]]\n",
    "        \n",
    "        # Create searchable text\n",
    "        search_text = ' '.join([\n",
    "            afsc.get('afscTitle', ''), \n",
    "            *afsc.get('onetTitles', []), \n",
    "            *afsc.get('socTitles', [])\n",
    "        ]).lower()\n",
    "        \n",
    "        # Add to processed entries\n",
    "        processed_entry = afsc.copy()\n",
    "        processed_entry['keywords'] = keywords\n",
    "        processed_entry['searchText'] = search_text\n",
    "        processed_entries.append(processed_entry)\n",
    "    \n",
    "    logger.info(f\"Keyword extraction complete. Processed {len(processed_entries)} entries.\")\n",
    "    \n",
    "    return {\n",
    "        'processedCrosswalk': processed_entries,\n",
    "        'metadata': {\n",
    "            **data['metadata'],\n",
    "            'keywordsExtracted': True\n",
    "        }\n",
    "    }\n",
    "\n",
    "def save_to_file(processed_data: Dict[str, Any], output_dir: str = '.', file_path: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Save processed data to a JSON file\n",
    "    \n",
    "    Args:\n",
    "        processed_data: The processed data to save\n",
    "        output_dir: Directory to save the file\n",
    "        file_path: Optional specific file path\n",
    "        \n",
    "    Returns:\n",
    "        Path to the saved file\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if file_path is None:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        file_path = os.path.join(output_dir, f\"processed-afsc-crosswalk-{timestamp}.json\")\n",
    "    \n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_data, f, indent=2)\n",
    "    \n",
    "    logger.info(f\"Data saved to {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point for the script\"\"\"\n",
    "    # URL to O*NET Crosswalk data\n",
    "    url = \"https://raw.githubusercontent.com/Kyleinexile/IS-Repo/refs/heads/main/ONET%20Crosswalk.csv\"\n",
    "\n",
    "    output_dir = \"processed_data\"\n",
    "    \n",
    "    try:\n",
    "        # Fetch data\n",
    "        csv_data = fetch_crosswalk_data(url)\n",
    "        \n",
    "        # Parse and filter data\n",
    "        parsed_data = parse_and_filter_csv(csv_data)\n",
    "        \n",
    "        # Extract keywords\n",
    "        enhanced_data = extract_keywords(parsed_data)\n",
    "        \n",
    "        # Save to file\n",
    "        save_to_file(enhanced_data, output_dir)\n",
    "        \n",
    "        logger.info(\"Workflow completed successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in workflow: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
