{
  "name": "IS Workflow 2.0",
  "nodes": [
    {
      "parameters": {},
      "type": "n8n-nodes-base.manualTrigger",
      "typeVersion": 1,
      "position": [
        -400,
        60
      ],
      "id": "45b3e345-d2ae-448e-b61b-20db563e2d35",
      "name": "When clicking ‘Test workflow’"
    },
    {
      "parameters": {
        "jsCode": "// Loop over input items and add a new field called 'myNewField' to the JSON of each one\nfor (const item of $input.all()) {\n  item.json.myNewField = 1;\n}\n// Process ESCO data with batching for reliability\nconst rawData = items[0].json.data;\n\n// Parse the JSON string if needed\nlet escoData;\ntry {\n  escoData = typeof rawData === 'string' ? JSON.parse(rawData) : rawData;\n} catch (error) {\n  console.log('Error parsing JSON:', error.message);\n  return [{ json: { error: 'Failed to parse ESCO data' } }];\n}\n\nconsole.log(`Total ESCO entries: ${escoData.length}`);\n\n// Define batch size\nconst batchSize = 1000;\nconst totalBatches = Math.ceil(escoData.length / batchSize);\n\n// Process only skills (not competences) for better performance\nconst skillEntries = escoData.filter(entry => \n  entry.skillType === 'skill/competence'\n);\n\nconsole.log(`Found ${skillEntries.length} skill/competence entries`);\n\n// Process in smaller batches\nconst processedSkills = [];\nfor (let i = 0; i < skillEntries.length; i += batchSize) {\n  const batchEnd = Math.min(i + batchSize, skillEntries.length);\n  console.log(`Processing batch ${Math.floor(i/batchSize) + 1}/${totalBatches}: entries ${i} to ${batchEnd-1}`);\n  \n  const batch = skillEntries.slice(i, batchEnd);\n  \n  // Process each skill in the batch\n  const processedBatch = batch.map(skill => {\n    try {\n      // Extract the verb (first word of skill name)\n      const skillNameWords = skill.skillName ? skill.skillName.trim().toLowerCase().split(/\\s+/) : [];\n      const verb = skillNameWords.length > 0 ? skillNameWords[0] : '';\n      \n      // Extract alternate labels\n      const alternateLabels = skill.alternateLabels ? \n        skill.alternateLabels.split('\\n').map(label => label.trim()).filter(Boolean) : \n        [];\n      \n      // Create a search text that combines all relevant fields\n      const searchText = [\n        skill.skillName || '',\n        alternateLabels.join(' '),\n        skill.description || ''\n      ].join(' ').toLowerCase();\n      \n      // Extract keywords from all text fields\n      const allWords = searchText.split(/\\s+/);\n      const commonWords = ['and', 'or', 'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'with', 'by', 'of', 'such', 'as'];\n      const filteredWords = allWords.filter(word => \n        word && word.length > 3 && !commonWords.includes(word)\n      );\n      \n      // Count word frequencies\n      const wordFreq = {};\n      filteredWords.forEach(word => {\n        wordFreq[word] = (wordFreq[word] || 0) + 1;\n      });\n      \n      // Sort by frequency and get top keywords\n      const keywords = Object.entries(wordFreq)\n        .sort((a, b) => b[1] - a[1])\n        .slice(0, 20)\n        .map(entry => entry[0]);\n      \n      return {\n        skillId: skill.id || '',\n        skillName: skill.skillName || '',\n        description: skill.description || '',\n        verb: verb,\n        alternateLabels: alternateLabels,\n        keywords: keywords,\n        searchText: searchText\n      };\n    } catch (error) {\n      console.log(`Error processing skill \"${skill.skillName}\": ${error.message}`);\n      return {\n        error: error.message,\n        skillName: skill.skillName || 'Unknown'\n      };\n    }\n  });\n  \n  processedSkills.push(...processedBatch);\n  console.log(`Batch complete. Total processed: ${processedSkills.length}/${skillEntries.length}`);\n}\n\nconsole.log(`ESCO processing complete. Processed ${processedSkills.length} skills.`);\n\n// Prepare the output\nconst outputData = {\n  processedEscoSkills: processedSkills,\n  metadata: {\n    totalEntries: escoData.length,\n    processedSkills: processedSkills.length,\n    processingDate: new Date().toISOString()\n  }\n};\n\n// Return the processed data for next node\nreturn [{\n  json: outputData\n}];\nreturn $input.all();"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -60,
        -160
      ],
      "id": "cab1b711-79c6-4d57-96fc-bbc68138b04e",
      "name": "ESCO Processing",
      "notes": "# What this node does\nProcesses the raw ESCO data to extract and structure skills information.\n\n# Input\nRaw ESCO JSON data from the HTTP Request node.\n\n# Processing\n1. Parses the JSON data\n2. Filters for entries with skillType = 'skill/competence'\n3. Processes data in batches to prevent memory issues\n4. For each skill:\n   - Extracts the verb (first word of skill name)\n   - Parses alternate labels\n   - Creates searchable text from all fields\n   - Identifies keywords based on frequency\n   - Structures data in a consistent format\n\n# Output\nStructured object containing processed skills and metadata."
    },
    {
      "parameters": {
        "url": "https://raw.githubusercontent.com/Kyleinexile/IS-Repo/refs/heads/main/ESCO.json",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        -220,
        -160
      ],
      "id": "6affb83d-1be2-4728-a92a-66a20614f17e",
      "name": "ESCO Github Pull",
      "notes": "# What this node does\nFetches raw ESCO skills data from the GitHub repository.\n\n# Input\nNone - this is the starting point of the workflow.\n\n# Processing\nSends an HTTP GET request to retrieve the ESCO JSON file.\n\n# Output\nRaw JSON data containing all ESCO entries."
    },
    {
      "parameters": {
        "url": "https://raw.githubusercontent.com/Kyleinexile/IS-Repo/refs/heads/main/ONET%20Crosswalk.csv",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        -220,
        60
      ],
      "id": "dc7e9b6e-adc8-42b8-9504-695bc2c35c57",
      "name": "ONet Github Pull",
      "notes": "# What this node does\nFetches O*NET Military Crosswalk CSV data from the GitHub repository.\n\n# Input\nNone - this is the starting point of the workflow.\n\n# Processing\nSends an HTTP GET request to retrieve the CSV file.\n\n# Output\nRaw CSV data containing military occupational codes and their civilian equivalents."
    },
    {
      "parameters": {
        "jsCode": "// Parse O*NET Crosswalk CSV and filter for Air Force entries\nconst csvData = items[0].json.data;\nconst rows = csvData.split('\\n');\nconst headers = rows[0].replace(/\"/g, '').split(',');\n\nconsole.log(`Processing ${rows.length} rows of crosswalk data`);\n\n// Process CSV data\nconst parsedData = [];\nfor (let i = 1; i < rows.length; i++) {\n  if (rows[i].trim() === '') continue;\n  \n  // Handle quoted CSV values accurately\n  let values = [];\n  let currentValue = '';\n  let inQuotes = false;\n  \n  for (let char of rows[i]) {\n    if (char === '\"') {\n      inQuotes = !inQuotes;\n    } else if (char === ',' && !inQuotes) {\n      values.push(currentValue);\n      currentValue = '';\n    } else {\n      currentValue += char;\n    }\n  }\n  values.push(currentValue); // Add the last value\n  \n  // Create entry object\n  const entry = {};\n  for (let j = 0; j < headers.length && j < values.length; j++) {\n    entry[headers[j]] = values[j].replace(/\"/g, '').trim();\n  }\n  \n  parsedData.push(entry);\n}\n\n// Count service types for diagnostic purposes\nconst serviceCounts = {};\nparsedData.forEach(item => {\n  const svc = item.SVC || 'unknown';\n  serviceCounts[svc] = (serviceCounts[svc] || 0) + 1;\n});\n\nconsole.log(\"Service type counts:\", JSON.stringify(serviceCounts));\n\n// Filter for Air Force entries (F, X, Y, Z)\nconst airForceData = parsedData.filter(item => \n  ['F', 'X', 'Y', 'Z'].includes(item.SVC) && \n  item.STATUS === 'A' // Only active records\n);\n\nconsole.log(`Found ${airForceData.length} Air Force related entries`);\nconsole.log(`SVC=F: ${airForceData.filter(item => item.SVC === 'F').length}`);\nconsole.log(`SVC=X: ${airForceData.filter(item => item.SVC === 'X').length}`);\nconsole.log(`SVC=Y: ${airForceData.filter(item => item.SVC === 'Y').length}`);\nconsole.log(`SVC=Z: ${airForceData.filter(item => item.SVC === 'Z').length}`);\n\n// Air Force Space Force (H, L, O, U) - these may be relevant if you want to include Space Force\nconst spaceForceData = parsedData.filter(item => \n  ['H', 'L', 'O', 'U'].includes(item.SVC) && \n  item.STATUS === 'A'\n);\n\nconsole.log(`Found ${spaceForceData.length} Space Force related entries`);\n\n// Map to the final format\nconst transformedData = airForceData.map(item => ({\n  afscCode: item.MOC,\n  afscTitle: item.MOC_TITLE,\n  serviceType: item.SVC,\n  category: item.MPC === 'E' ? 'Enlisted' : (item.MPC === 'O' ? 'Officer' : 'Warrant'),\n  onetCodes: [item.ONET1, item.ONET2, item.ONET3, item.ONET4].filter(Boolean),\n  onetTitles: [item.ONET1_TITLE, item.ONET2_TITLE, item.ONET3_TITLE, item.ONET4_TITLE].filter(Boolean),\n  socCodes: [item.SOC1, item.SOC2, item.SOC3, item.SOC4].filter(Boolean),\n  socTitles: [item.SOC1_TITLE, item.SOC2_TITLE, item.SOC3_TITLE, item.SOC4_TITLE].filter(Boolean),\n  statusDate: item.SDATE || '',\n  endDate: item.EDATE || ''\n}));\n\n// Include Space Force entries as well if any (they're closely related)\nif (spaceForceData.length > 0) {\n  console.log(\"Including Space Force entries as well.\");\n  \n  const spaceForceTransformed = spaceForceData.map(item => ({\n    afscCode: item.MOC,\n    afscTitle: item.MOC_TITLE,\n    serviceType: item.SVC,\n    category: item.MPC === 'E' ? 'Enlisted' : (item.MPC === 'O' ? 'Officer' : 'Warrant'),\n    isSpaceForce: true,\n    onetCodes: [item.ONET1, item.ONET2, item.ONET3, item.ONET4].filter(Boolean),\n    onetTitles: [item.ONET1_TITLE, item.ONET2_TITLE, item.ONET3_TITLE, item.ONET4_TITLE].filter(Boolean),\n    socCodes: [item.SOC1, item.SOC2, item.SOC3, item.SOC4].filter(Boolean),\n    socTitles: [item.SOC1_TITLE, item.SOC2_TITLE, item.SOC3_TITLE, item.SOC4_TITLE].filter(Boolean),\n    statusDate: item.SDATE || '',\n    endDate: item.EDATE || ''\n  }));\n  \n  transformedData.push(...spaceForceTransformed);\n}\n\nconsole.log(`Total processed entries: ${transformedData.length}`);\n\nreturn [{\n  json: {\n    processedCrosswalk: transformedData,\n    metadata: {\n      totalEntries: parsedData.length,\n      airForceEntries: airForceData.length,\n      spaceForceEntries: spaceForceData.length,\n      processingDate: new Date().toISOString()\n    }\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -60,
        60
      ],
      "id": "2a8728bf-7c0a-4f28-a1c6-78e49f2a81e1",
      "name": "ONet Parse/Filter",
      "notes": "# What this node does\nParses the CSV data and filters for Air Force specialty codes.\n\n# Input\nRaw CSV data from the HTTP Request node.\n\n# Processing\n1. Parses the CSV format with proper handling of quoted values\n2. Extracts header information\n3. Filters for Air Force entries (SVC codes F, X, Y, Z)\n4. Also collects Space Force entries (SVC codes H, L, O, U)\n5. Transforms the data into a structured format with key fields\n\n# Output\nObject containing filtered and structured AFSC data with metadata."
    },
    {
      "parameters": {
        "jsCode": "// Extract keywords and create search text for AFSCs\nconst data = items[0].json;\nconst afscEntries = data.processedCrosswalk;\n\nconsole.log(`Processing ${afscEntries.length} AFSC entries for keyword extraction`);\n\nconst processedEntries = afscEntries.map(afsc => {\n  // Extract keywords from AFSC title and O*NET titles\n  let titleWords = afsc.afscTitle ? afsc.afscTitle.toLowerCase().split(/\\s+/) : [];\n  let onetWords = [];\n  \n  // Extract words from all O*NET titles\n  if (afsc.onetTitles && afsc.onetTitles.length > 0) {\n    afsc.onetTitles.forEach(title => {\n      if (title) {\n        onetWords = [...onetWords, ...title.toLowerCase().split(/\\s+/)];\n      }\n    });\n  }\n  \n  // Extract words from all SOC titles\n  let socWords = [];\n  if (afsc.socTitles && afsc.socTitles.length > 0) {\n    afsc.socTitles.forEach(title => {\n      if (title) {\n        socWords = [...socWords, ...title.toLowerCase().split(/\\s+/)];\n      }\n    });\n  }\n  \n  // Combine all words and remove common words\n  const commonWords = ['and', 'or', 'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'with', 'by', 'of'];\n  const allWords = [...titleWords, ...onetWords, ...socWords].filter(word => \n    word && word.length > 2 && !commonWords.includes(word)\n  );\n  \n  // Count word frequencies\n  const wordFreq = {};\n  allWords.forEach(word => {\n    wordFreq[word] = (wordFreq[word] || 0) + 1;\n  });\n  \n  // Sort by frequency and get top keywords\n  const keywords = Object.entries(wordFreq)\n    .sort((a, b) => b[1] - a[1])\n    .slice(0, 30)\n    .map(entry => entry[0]);\n  \n  // Create a search text that combines all relevant fields\n  const searchText = [\n    afsc.afscTitle || '',\n    ...(afsc.onetTitles || []),\n    ...(afsc.socTitles || [])\n  ].join(' ').toLowerCase();\n  \n  return {\n    ...afsc,\n    keywords,\n    searchText\n  };\n});\n\nconsole.log(`Keyword extraction complete. Processed ${processedEntries.length} entries.`);\n\nreturn [{\n  json: {\n    processedCrosswalk: processedEntries,\n    metadata: {\n      ...data.metadata,\n      keywordsExtracted: true\n    }\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        100,
        60
      ],
      "id": "ea0ad477-2cb3-4c03-9c25-5f21c06b8cda",
      "name": "Keyword Extraction",
      "notes": "# What this node does\nAnalyzes AFSC titles and related occupational titles to extract meaningful keywords.\n\n# Input\nStructured AFSC data from the previous node.\n\n# Processing\n1. Extracts words from AFSC titles, O*NET titles, and SOC titles\n2. Removes common words and short terms\n3. Counts word frequencies to identify the most significant terms\n4. Creates a searchable text field combining all relevant information\n\n# Output\nEnhanced AFSC data with keywords and search text for later matching with skills."
    },
    {
      "parameters": {
        "authentication": "oAuth2",
        "resource": "file",
        "owner": {
          "__rl": true,
          "value": "Kyleinexile",
          "mode": "list",
          "cachedResultName": "Kyleinexile",
          "cachedResultUrl": "https://github.com/Kyleinexile"
        },
        "repository": {
          "__rl": true,
          "value": "IS-Repo",
          "mode": "list",
          "cachedResultName": "IS-Repo",
          "cachedResultUrl": "https://github.com/Kyleinexile/IS-Repo"
        },
        "filePath": "{{$node[\"ONet GH Prep\"].json.filePath}}",
        "fileContent": "={{$node[\"ONet GH Prep\"].json.content}}\n",
        "commitMessage": "{{$node[\"ONet GH Prep\"].json.commitMessage}}"
      },
      "type": "n8n-nodes-base.github",
      "typeVersion": 1,
      "position": [
        420,
        60
      ],
      "id": "46a20057-c5d5-45c2-aa96-12dc6b4f6151",
      "name": "GitHub ONet Upload",
      "credentials": {
        "githubOAuth2Api": {
          "id": "ZTHP0fnC7Mg1hvxD",
          "name": "GitHub account"
        }
      },
      "notes": "# What this node does\nSaves the processed AFSC data to GitHub as a JSON file.\n\n# Input\nGitHub parameters from the previous node (content, path, message).\n\n# Processing\n1. Authenticates with GitHub\n2. Creates a new file in the repository\n3. Writes the processed data with the provided filename\n\n# Output\nStatus information from the GitHub API response."
    },
    {
      "parameters": {
        "authentication": "oAuth2",
        "resource": "file",
        "owner": {
          "__rl": true,
          "value": "Kyleinexile",
          "mode": "list",
          "cachedResultName": "Kyleinexile",
          "cachedResultUrl": "https://github.com/Kyleinexile"
        },
        "repository": {
          "__rl": true,
          "value": "IS-Repo",
          "mode": "list",
          "cachedResultName": "IS-Repo",
          "cachedResultUrl": "https://github.com/Kyleinexile/IS-Repo"
        },
        "filePath": "{{$node[\"GH Prep\"].json.filePath}}",
        "fileContent": "={{$node[\"ESCO GH Prep\"].json.content}}\n",
        "commitMessage": "{{$node[\"GH Prep\"].json.commitMessage}}"
      },
      "type": "n8n-nodes-base.github",
      "typeVersion": 1,
      "position": [
        260,
        -160
      ],
      "id": "6b2c94e3-65f0-4e8a-b6e1-18680eb42b55",
      "name": "GitHub ESCO Upload",
      "credentials": {
        "githubOAuth2Api": {
          "id": "ZTHP0fnC7Mg1hvxD",
          "name": "GitHub account"
        }
      },
      "notes": "# What this node does\nSaves the processed ESCO data to GitHub as a JSON file.\n\n# Input\nGitHub parameters from the previous node (content, path, message).\n\n# Processing\n1. Authenticates with GitHub\n2. Creates a new file in the repository\n3. Writes the processed data with the provided filename\n\n# Output\nStatus information from the GitHub API response."
    },
    {
      "parameters": {
        "jsCode": "// Prepare for GitHub storage with improved error handling\n// First, safely get the processed data\nlet processedData;\ntry {\n  processedData = items[0]?.json || {};\n  \n  console.log(\"Data received from previous node:\", \n    JSON.stringify({\n      type: typeof processedData,\n      hasProcessedEscoSkills: !!processedData.processedEscoSkills,\n      metadata: processedData.metadata || \"Not present\"\n    })\n  );\n  \n  // Ensure processedEscoSkills exists and is an array\n  if (!processedData.processedEscoSkills) {\n    console.log(\"No processedEscoSkills found, creating empty array\");\n    processedData.processedEscoSkills = [];\n  }\n  \n  // Ensure metadata exists\n  if (!processedData.metadata) {\n    console.log(\"No metadata found, creating default metadata\");\n    processedData.metadata = {\n      totalEntries: 0,\n      processedSkills: processedData.processedEscoSkills.length,\n      processingDate: new Date().toISOString()\n    };\n  }\n} catch (error) {\n  console.log(\"Error accessing input data:\", error.message);\n  // Create fallback data\n  processedData = {\n    processedEscoSkills: [],\n    metadata: {\n      error: \"Failed to process input data: \" + error.message,\n      processingDate: new Date().toISOString()\n    }\n  };\n}\n\n// Generate a JSON string with processed data\nconst jsonString = JSON.stringify(processedData, null, 2);\n\n// Create a timestamp for the filename\nconst timestamp = new Date().toISOString().replace(/:/g, '-').replace(/\\..+/, '');\n\nreturn [{\n  json: {\n    content: jsonString,\n    filePath: `processed-esco-skills-${timestamp}.json`,\n    commitMessage: \"Add processed ESCO skills data\"\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        100,
        -160
      ],
      "id": "74148f6d-8427-4413-998b-c9d6a02f8dde",
      "name": "ESCO GH Prep",
      "notes": "# What this node does\nPrepares the processed ESCO data for GitHub storage.\n\n# Input\nStructured object containing processed skills from the previous node.\n\n# Processing\n1. Creates a JSON string from the processed data\n2. Generates a unique filename using the current timestamp\n3. Prepares the necessary GitHub parameters (content, path, message)\n\n# Output\nObject containing:\n- content: JSON string to store\n- filePath: Unique filename for GitHub storage\n- commitMessage: Description of the update"
    },
    {
      "parameters": {
        "jsCode": "// Prepare Crosswalk data for GitHub storage\nconst data = items[0].json;\n\n// Generate a JSON string with processed data\nconst jsonString = JSON.stringify(data, null, 2);\n\n// Create a timestamp with milliseconds for a truly unique filename\nconst timestamp = Date.now(); // This gives milliseconds since epoch\n\n// Make sure the folder path matches exactly\nconst folderPath = \"ONET-Crosswalk-Workflow\";\nconst fileName = `processed-afsc-crosswalk-${timestamp}.json`;\nconst filePath = `${folderPath}/${fileName}`;\n\nconsole.log(`Preparing to save file to: ${filePath}`);\n\nreturn [{\n  json: {\n    content: jsonString,\n    filePath: filePath,\n    commitMessage: \"Add processed O*NET Military Crosswalk data\"\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        260,
        60
      ],
      "id": "edbc8ab7-fb00-40d3-a948-198eb95d00b2",
      "name": "ONet GH Prep",
      "notes": "# What this node does\nPrepares the processed AFSC data for GitHub storage.\n\n# Input\nEnhanced AFSC data with keywords from the previous node.\n\n# Processing\n1. Creates a JSON string from the processed data\n2. Generates a unique filename using the current timestamp\n3. Prepares the necessary GitHub parameters (content, path, message)\n\n# Output\nObject containing content, filePath, and commitMessage for the GitHub node."
    }
  ],
  "pinData": {},
  "connections": {
    "When clicking ‘Test workflow’": {
      "main": [
        [
          {
            "node": "ONet Github Pull",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "ESCO Processing": {
      "main": [
        [
          {
            "node": "ESCO GH Prep",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "ESCO Github Pull": {
      "main": [
        [
          {
            "node": "ESCO Processing",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "ONet Github Pull": {
      "main": [
        [
          {
            "node": "ONet Parse/Filter",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "ONet Parse/Filter": {
      "main": [
        [
          {
            "node": "Keyword Extraction",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Keyword Extraction": {
      "main": [
        [
          {
            "node": "ONet GH Prep",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "ESCO GH Prep": {
      "main": [
        [
          {
            "node": "GitHub ESCO Upload",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "ONet GH Prep": {
      "main": [
        [
          {
            "node": "GitHub ONet Upload",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "1370ef47-710e-403f-800d-992886024b74",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "43f1f94ffedd314979634e3905a489c007c9c4f186960aec85160db74a936486"
  },
  "id": "0JqdW4gul88Rd8kb",
  "tags": []
}