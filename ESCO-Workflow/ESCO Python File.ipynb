{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 13:32:53,445 - INFO - Fetching ESCO data from https://raw.githubusercontent.com/Kyleinexile/IS-Repo/refs/heads/main/ESCO.json\n",
      "2025-03-28 13:32:54,552 - INFO - Retrieved 13939 ESCO entries\n",
      "2025-03-28 13:32:54,554 - INFO - Found 10715 skill/competence entries\n",
      "2025-03-28 13:32:54,554 - INFO - Processing batch 1/11: entries 0 to 999\n",
      "2025-03-28 13:32:54,573 - INFO - Processing batch 2/11: entries 1000 to 1999\n",
      "2025-03-28 13:32:54,592 - INFO - Processing batch 3/11: entries 2000 to 2999\n",
      "2025-03-28 13:32:54,613 - INFO - Processing batch 4/11: entries 3000 to 3999\n",
      "2025-03-28 13:32:54,632 - INFO - Processing batch 5/11: entries 4000 to 4999\n",
      "2025-03-28 13:32:54,652 - INFO - Processing batch 6/11: entries 5000 to 5999\n",
      "2025-03-28 13:32:54,672 - INFO - Processing batch 7/11: entries 6000 to 6999\n",
      "2025-03-28 13:32:54,692 - INFO - Processing batch 8/11: entries 7000 to 7999\n",
      "2025-03-28 13:32:54,712 - INFO - Processing batch 9/11: entries 8000 to 8999\n",
      "2025-03-28 13:32:54,732 - INFO - Processing batch 10/11: entries 9000 to 9999\n",
      "2025-03-28 13:32:54,751 - INFO - Processing batch 11/11: entries 10000 to 10714\n",
      "2025-03-28 13:32:54,764 - INFO - ESCO processing complete. Processed 10715 skills.\n",
      "2025-03-28 13:32:54,952 - INFO - Data saved to processed_data\\processed-esco-skills-2025-03-28-13-32-54.json\n",
      "2025-03-28 13:32:54,953 - INFO - Workflow completed successfully\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ESCO Skills Data Processing Pipeline\n",
    "\n",
    "Replicates the final n8n-style pipeline in a single Python script:\n",
    "1. Fetches raw ESCO data from GitHub (JSON).\n",
    "2. Filters for skillType = 'skill/competence'.\n",
    "3. Maps conceptUri -> skillId, preferredLabel -> skillName.\n",
    "4. Splits altLabels on newlines into an array.\n",
    "5. Extracts a 'verb' (first word of the skillName).\n",
    "6. Combines skillName, altLabels, description into 'searchText' (lowercased).\n",
    "7. Generates up to 20 high-frequency keywords from searchText.\n",
    "8. Saves the processed data into a 'processed_data' folder.\n",
    "\n",
    "Usage:\n",
    "    python esco_processor.py\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import re\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import logging\n",
    "\n",
    "# Configure basic logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def fetch_esco_data(url: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetch ESCO data (JSON list) from a provided URL (GitHub raw link).\n",
    "    \"\"\"\n",
    "    logger.info(f\"Fetching ESCO data from {url}\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    data = json.loads(response.text)\n",
    "    logger.info(f\"Retrieved {len(data)} ESCO entries\")\n",
    "    return data\n",
    "\n",
    "def process_esco_skills(esco_data: List[Dict[str, Any]], batch_size: int = 1000) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process ESCO data to match the n8n pipeline's final structure:\n",
    "      - Keep conceptUri as skillId\n",
    "      - Convert preferredLabel -> skillName\n",
    "      - Preserve skillType, reuseLevel, description\n",
    "      - altLabels -> array of lines\n",
    "      - verb is the first word of skillName (lowercased)\n",
    "      - searchText is skillName + altLabels + description\n",
    "      - Up to 20 keywords, ignoring common short words\n",
    "    \"\"\"\n",
    "    # Filter: only keep skillType = 'skill/competence'\n",
    "    skill_entries = [entry for entry in esco_data if entry.get('skillType') == 'skill/competence']\n",
    "    logger.info(f\"Found {len(skill_entries)} skill/competence entries\")\n",
    "\n",
    "    total_batches = (len(skill_entries) + batch_size - 1) // batch_size\n",
    "    processed_skills = []\n",
    "\n",
    "    for i in range(0, len(skill_entries), batch_size):\n",
    "        batch_end = min(i + batch_size, len(skill_entries))\n",
    "        logger.info(f\"Processing batch {i // batch_size + 1}/{total_batches}: entries {i} to {batch_end - 1}\")\n",
    "        batch = skill_entries[i:batch_end]\n",
    "\n",
    "        for skill in batch:\n",
    "            try:\n",
    "                # conceptUri -> skillId\n",
    "                skill_id = skill.get('conceptUri', '')\n",
    "\n",
    "                # preferredLabel -> skillName\n",
    "                raw_name = skill.get('preferredLabel', '').strip()\n",
    "                skill_name_lower = raw_name.lower()\n",
    "\n",
    "                # verb -> first word of skillName\n",
    "                skill_name_words = re.split(r'\\s+', skill_name_lower)\n",
    "                verb = skill_name_words[0] if skill_name_words else ''\n",
    "\n",
    "                # altLabels -> array of synonyms (split on newlines)\n",
    "                alt_labels_text = skill.get('altLabels', '')\n",
    "                alternate_labels = [lbl.strip() for lbl in alt_labels_text.split('\\n') if lbl.strip()]\n",
    "\n",
    "                # Build searchText from skillName + altLabels + description\n",
    "                description_text = skill.get('description', '')\n",
    "                search_text = ' '.join([\n",
    "                    raw_name,\n",
    "                    ' '.join(alternate_labels),\n",
    "                    description_text\n",
    "                ]).lower()\n",
    "\n",
    "                # Remove short words & common stopwords, then keep top 20 frequencies\n",
    "                all_words = re.split(r'\\s+', search_text)\n",
    "                common_words = ['and','or','the','a','an','in','on','at','to','for','with','by','of','such','as']\n",
    "                filtered_words = [w for w in all_words if w and len(w) > 3 and w not in common_words]\n",
    "\n",
    "                word_freq = {}\n",
    "                for w in filtered_words:\n",
    "                    word_freq[w] = word_freq.get(w, 0) + 1\n",
    "\n",
    "                # Sort by frequency desc, slice top 20\n",
    "                keywords = [w for w, _ in sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:20]]\n",
    "\n",
    "                processed_skills.append({\n",
    "                    'skillId': skill_id,\n",
    "                    'skillType': skill.get('skillType', ''),\n",
    "                    'reuseLevel': skill.get('reuseLevel', ''),\n",
    "                    'skillName': raw_name,\n",
    "                    'description': description_text,\n",
    "                    'verb': verb,\n",
    "                    'alternateLabels': alternate_labels,\n",
    "                    'keywords': keywords,\n",
    "                    'searchText': search_text\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing skill '{skill.get('preferredLabel', 'Unknown')}': {str(e)}\")\n",
    "\n",
    "    logger.info(f\"ESCO processing complete. Processed {len(processed_skills)} skills.\")\n",
    "\n",
    "    return {\n",
    "        'processedEscoSkills': processed_skills,\n",
    "        'metadata': {\n",
    "            'totalEntries': len(esco_data),\n",
    "            'processedSkills': len(processed_skills),\n",
    "            'processingDate': datetime.datetime.now().isoformat()\n",
    "        }\n",
    "    }\n",
    "\n",
    "def save_to_file(processed_data: Dict[str, Any], output_dir: str = 'processed_data', file_path: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Save processed data to a JSON file in the specified directory.\n",
    "    If file_path is not provided, creates a timestamped filename.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if file_path is None:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        file_path = os.path.join(output_dir, f\"processed-esco-skills-{timestamp}.json\")\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_data, f, indent=2)\n",
    "\n",
    "    logger.info(f\"Data saved to {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the ESCO processing pipeline.\n",
    "    \"\"\"\n",
    "    # URL to ESCO data (GitHub raw link)\n",
    "    url = \"https://raw.githubusercontent.com/Kyleinexile/IS-Repo/refs/heads/main/ESCO.json\"\n",
    "\n",
    "    try:\n",
    "        # 1. Fetch data\n",
    "        esco_data = fetch_esco_data(url)\n",
    "\n",
    "        # 2. Process data\n",
    "        processed_data = process_esco_skills(esco_data)\n",
    "\n",
    "        # 3. Save to file in processed_data folder\n",
    "        save_to_file(processed_data, 'processed_data')\n",
    "\n",
    "        logger.info(\"Workflow completed successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in workflow: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
